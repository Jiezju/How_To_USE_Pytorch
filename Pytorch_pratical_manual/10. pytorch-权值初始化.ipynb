{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 权值初始化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度爆炸实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T12:07:46.149446Z",
     "start_time": "2020-04-25T12:07:45.747156Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T12:10:17.681267Z",
     "start_time": "2020-04-25T12:10:17.674285Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data)    # normal: mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T12:11:58.915800Z",
     "start_time": "2020-04-25T12:11:58.080971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)  # 输出为NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过使用合适的初始化方差避免梯度爆炸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T12:40:45.780751Z",
     "start_time": "2020-04-25T12:40:45.774767Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, std=np.sqrt(1/self.neural_num))  # std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T12:40:46.405082Z",
     "start_time": "2020-04-25T12:40:46.177691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0790,  0.5840, -0.2799,  ...,  0.1225,  0.2502,  0.1782],\n",
      "        [-0.2215,  0.8377, -0.5547,  ...,  0.2425, -0.7018,  1.1625],\n",
      "        [ 0.2962,  0.0258, -0.0139,  ..., -0.0770, -0.1920, -0.2415],\n",
      "        ...,\n",
      "        [-0.3592,  0.2852, -0.4397,  ..., -0.2645, -0.6763, -0.2311],\n",
      "        [ 0.4422,  0.5176,  0.0465,  ...,  0.0168,  0.3626,  0.4444],\n",
      "        [-0.5831, -1.7075,  0.4821,  ...,  0.4343, -1.6414, -1.0328]],\n",
      "       grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)  # 输出为NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度消失现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T12:43:36.913312Z",
     "start_time": "2020-04-25T12:43:36.907329Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(neural_num, neural_num, bias=False)\n",
    "            for i in range(layers)\n",
    "        ])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = torch.relu(x)  # 导致梯度消失\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, std=np.sqrt(1 / self.neural_num))  # std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T12:43:37.388046Z",
     "start_time": "2020-04-25T12:43:37.238444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 7.6612e-16, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 6.2243e-16, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 5.8723e-16, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 5.3192e-16, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 7.0749e-16, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 6.4460e-16, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)  # 输出接近于0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**增益计算**\n",
    "\n",
    "    torch.nn.init.caculate_gain(nonlinearity, param=None)\n",
    "    \n",
    "    - 计算特定激活函数方差的变化尺度\n",
    "    \n",
    "    - nonlinearity  激活函数名称\n",
    "    \n",
    "    - param    激活函数的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T13:26:14.856918Z",
     "start_time": "2020-04-25T13:26:14.728231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gain:1.6008168458938599\n",
      "tanh_gain in PyTorch: 1.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10000)\n",
    "out = torch.tanh(x)\n",
    "\n",
    "gain = x.std() / out.std()\n",
    "print('gain:{}'.format(gain))\n",
    "\n",
    "tanh_gain = nn.init.calculate_gain('tanh')\n",
    "print('tanh_gain in PyTorch:', tanh_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xavier初始化方法**\n",
    "\n",
    "    torch.nn.init.xavier_uniform_(tensor, gain=1)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(tensor, gain=1)\n",
    "    \n",
    "**功能**\n",
    "\n",
    "    维持梯度保持在固定区间内\n",
    "    \n",
    "**原理**\n",
    "\n",
    "    使用设定的范围初始化均匀分布，一般不适合与relu结合使用\n",
    "\n",
    "\n",
    "**初始化的实现**\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # Xavier初始化原理\n",
    "                a = np.sqrt(6 / (self.neural_num + self.neural_num))\n",
    "                tanh_gain = nn.init.calculate_gain('tanh')\n",
    "                a *= tanh_gain\n",
    "                nn.init.uniform_(m.weight.data, -a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T13:02:16.292854Z",
     "start_time": "2020-04-25T13:02:16.287864Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = torch.relu(x)  # 配合 relu\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                tanh_gain = nn.init.calculate_gain('relu')\n",
    "                nn.init.xavier_uniform_(m.weight.data, gain=tanh_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T13:02:16.906237Z",
     "start_time": "2020-04-25T13:02:16.780251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6384, 0.4085, 0.1534,  ..., 1.2654, 0.0000, 0.2298],\n",
      "        [0.9316, 0.1920, 0.1118,  ..., 0.7108, 0.0000, 0.1555],\n",
      "        [0.7651, 0.2095, 0.0279,  ..., 0.5919, 0.0000, 0.1411],\n",
      "        ...,\n",
      "        [0.8130, 0.1709, 0.0551,  ..., 0.6211, 0.0000, 0.1722],\n",
      "        [1.3152, 0.3116, 0.1029,  ..., 0.9901, 0.0000, 0.1942],\n",
      "        [1.3572, 0.3087, 0.0561,  ..., 1.0867, 0.0000, 0.2738]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)  # 输出接近于0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kaiming初始化方法**\n",
    "\n",
    "    torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    \n",
    "    torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    \n",
    "**特点**\n",
    "\n",
    "    基于relu激活函数提出的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T13:09:53.479474Z",
     "start_time": "2020-04-25T13:09:53.474486Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = torch.relu(x)  # 配合 relu\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T13:10:27.594365Z",
     "start_time": "2020-04-25T13:10:27.447765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.1933, 0.0000,  ..., 0.2536, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2283, 0.0000,  ..., 0.2499, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2110, 0.0000,  ..., 0.2771, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.2107, 0.0000,  ..., 0.2763, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2317, 0.0000,  ..., 0.3331, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2147, 0.0000,  ..., 0.2976, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)  # 输出接近于0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**均匀分布**\n",
    "\n",
    "    torch.nn.init.uniform_(tensor, a=0, b=1)\n",
    "    \n",
    "**正态分布**\n",
    "\n",
    "    torch.nn.init.normal_(tensor, mean=0, std=1)\n",
    "    \n",
    "**常数**\n",
    "\n",
    "    torch.nn.init.constant_(tensor, val)\n",
    "    \n",
    "**单位矩阵初始化**\n",
    "\n",
    "    torch.nn.init.eye_(tensor)\n",
    "    \n",
    "**正交初始化**\n",
    "\n",
    "    torch.nn.init.orthogonal_(tensor, gain=1)\n",
    "    \n",
    "**稀疏初始化**\n",
    "\n",
    "    torch.nn.init.sparse_(tensor, sparsity, std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 单层初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T18:17:05.986275Z",
     "start_time": "2020-03-24T18:17:05.773837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.0175,  0.0093,  0.0396,  ...,  0.0313, -0.0003,  0.0153],\n",
      "          [-0.0039, -0.0056,  0.0201,  ...,  0.0214, -0.0345,  0.0068],\n",
      "          [-0.0064,  0.0138, -0.0350,  ..., -0.0128, -0.0297, -0.0345],\n",
      "          ...,\n",
      "          [ 0.0383,  0.0420, -0.0271,  ..., -0.0331,  0.0039, -0.0321],\n",
      "          [-0.0080, -0.0219,  0.0263,  ..., -0.0294,  0.0237, -0.0319],\n",
      "          [-0.0212,  0.0002,  0.0208,  ...,  0.0399, -0.0014,  0.0211]],\n",
      "\n",
      "         [[ 0.0337, -0.0185, -0.0076,  ..., -0.0284,  0.0043, -0.0256],\n",
      "          [-0.0373, -0.0103,  0.0378,  ..., -0.0105, -0.0104,  0.0053],\n",
      "          [ 0.0426, -0.0061, -0.0005,  ..., -0.0348, -0.0038, -0.0127],\n",
      "          ...,\n",
      "          [ 0.0080,  0.0298,  0.0316,  ..., -0.0164, -0.0237,  0.0207],\n",
      "          [-0.0329,  0.0420, -0.0300,  ..., -0.0288,  0.0389, -0.0265],\n",
      "          [-0.0415, -0.0118, -0.0039,  ..., -0.0111,  0.0357,  0.0143]],\n",
      "\n",
      "         [[ 0.0122, -0.0120,  0.0289,  ..., -0.0065, -0.0219, -0.0176],\n",
      "          [-0.0393, -0.0130,  0.0328,  ..., -0.0253,  0.0234,  0.0076],\n",
      "          [-0.0244,  0.0418,  0.0203,  ..., -0.0131, -0.0156, -0.0254],\n",
      "          ...,\n",
      "          [ 0.0117, -0.0252, -0.0276,  ...,  0.0427,  0.0192,  0.0306],\n",
      "          [ 0.0412,  0.0069, -0.0360,  ...,  0.0038,  0.0400, -0.0248],\n",
      "          [-0.0065, -0.0426, -0.0053,  ...,  0.0216, -0.0224, -0.0022]]],\n",
      "\n",
      "\n",
      "        [[[-0.0364,  0.0415,  0.0037,  ..., -0.0333, -0.0404,  0.0142],\n",
      "          [-0.0035,  0.0160, -0.0116,  ...,  0.0141, -0.0274, -0.0272],\n",
      "          [-0.0006,  0.0160, -0.0196,  ...,  0.0300, -0.0306, -0.0131],\n",
      "          ...,\n",
      "          [ 0.0362,  0.0100, -0.0261,  ...,  0.0422, -0.0217,  0.0336],\n",
      "          [-0.0314, -0.0069,  0.0199,  ...,  0.0103,  0.0110,  0.0204],\n",
      "          [-0.0407,  0.0137, -0.0268,  ..., -0.0364,  0.0350,  0.0099]],\n",
      "\n",
      "         [[ 0.0344, -0.0143, -0.0387,  ..., -0.0146, -0.0100,  0.0127],\n",
      "          [ 0.0403,  0.0241,  0.0195,  ...,  0.0261, -0.0028,  0.0422],\n",
      "          [-0.0368,  0.0194, -0.0425,  ..., -0.0106, -0.0193, -0.0394],\n",
      "          ...,\n",
      "          [-0.0202, -0.0426, -0.0038,  ...,  0.0064,  0.0079, -0.0165],\n",
      "          [ 0.0346, -0.0263,  0.0159,  ...,  0.0374,  0.0133, -0.0191],\n",
      "          [ 0.0088, -0.0119, -0.0082,  ..., -0.0418, -0.0180, -0.0308]],\n",
      "\n",
      "         [[-0.0215, -0.0185,  0.0260,  ..., -0.0321,  0.0170,  0.0412],\n",
      "          [ 0.0303, -0.0097, -0.0279,  ..., -0.0195,  0.0250, -0.0054],\n",
      "          [-0.0105,  0.0229, -0.0210,  ..., -0.0390, -0.0235,  0.0131],\n",
      "          ...,\n",
      "          [-0.0303, -0.0225, -0.0111,  ...,  0.0078, -0.0421,  0.0008],\n",
      "          [ 0.0259, -0.0396,  0.0286,  ...,  0.0313, -0.0281, -0.0382],\n",
      "          [ 0.0112,  0.0157,  0.0200,  ...,  0.0310,  0.0273,  0.0013]]],\n",
      "\n",
      "\n",
      "        [[[-0.0008,  0.0233, -0.0311,  ...,  0.0073,  0.0314,  0.0125],\n",
      "          [-0.0308,  0.0395, -0.0190,  ..., -0.0379, -0.0381,  0.0287],\n",
      "          [-0.0402,  0.0045,  0.0195,  ...,  0.0020,  0.0238, -0.0163],\n",
      "          ...,\n",
      "          [-0.0120,  0.0170,  0.0133,  ...,  0.0335,  0.0030, -0.0235],\n",
      "          [-0.0352, -0.0235, -0.0128,  ...,  0.0420, -0.0048,  0.0315],\n",
      "          [-0.0353,  0.0018, -0.0022,  ..., -0.0135,  0.0051, -0.0055]],\n",
      "\n",
      "         [[-0.0403,  0.0009, -0.0212,  ...,  0.0159,  0.0001, -0.0057],\n",
      "          [ 0.0339, -0.0055,  0.0047,  ...,  0.0390, -0.0104,  0.0295],\n",
      "          [-0.0355, -0.0006,  0.0344,  ..., -0.0358,  0.0147, -0.0419],\n",
      "          ...,\n",
      "          [-0.0046,  0.0087, -0.0169,  ...,  0.0056,  0.0229, -0.0163],\n",
      "          [-0.0285, -0.0386, -0.0226,  ..., -0.0381, -0.0155, -0.0149],\n",
      "          [-0.0401,  0.0370,  0.0357,  ..., -0.0287, -0.0253,  0.0299]],\n",
      "\n",
      "         [[-0.0132,  0.0313,  0.0091,  ...,  0.0183,  0.0354,  0.0008],\n",
      "          [-0.0285, -0.0416,  0.0272,  ..., -0.0175,  0.0112,  0.0002],\n",
      "          [ 0.0241, -0.0426,  0.0380,  ..., -0.0193,  0.0392, -0.0107],\n",
      "          ...,\n",
      "          [-0.0121,  0.0067, -0.0073,  ...,  0.0133, -0.0316,  0.0030],\n",
      "          [ 0.0421,  0.0350,  0.0065,  ...,  0.0061, -0.0420, -0.0199],\n",
      "          [ 0.0228,  0.0265, -0.0246,  ...,  0.0233, -0.0392,  0.0305]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0325, -0.0364, -0.0036,  ...,  0.0361,  0.0397,  0.0409],\n",
      "          [ 0.0356,  0.0120,  0.0146,  ..., -0.0111,  0.0364, -0.0190],\n",
      "          [-0.0038,  0.0240, -0.0293,  ..., -0.0292,  0.0184,  0.0095],\n",
      "          ...,\n",
      "          [-0.0087,  0.0214, -0.0214,  ..., -0.0128,  0.0412, -0.0233],\n",
      "          [ 0.0132, -0.0233, -0.0352,  ...,  0.0024,  0.0108, -0.0252],\n",
      "          [ 0.0332,  0.0028,  0.0378,  ...,  0.0061, -0.0168,  0.0030]],\n",
      "\n",
      "         [[ 0.0346, -0.0078,  0.0066,  ...,  0.0137,  0.0382,  0.0033],\n",
      "          [ 0.0327, -0.0198, -0.0167,  ...,  0.0169, -0.0114, -0.0351],\n",
      "          [ 0.0233,  0.0382,  0.0330,  ..., -0.0409,  0.0062,  0.0152],\n",
      "          ...,\n",
      "          [ 0.0067,  0.0383, -0.0350,  ..., -0.0162,  0.0212, -0.0206],\n",
      "          [-0.0118, -0.0055, -0.0160,  ...,  0.0219,  0.0177,  0.0036],\n",
      "          [ 0.0287, -0.0166,  0.0210,  ...,  0.0257, -0.0305,  0.0016]],\n",
      "\n",
      "         [[ 0.0401,  0.0219,  0.0316,  ..., -0.0227,  0.0092, -0.0036],\n",
      "          [ 0.0216, -0.0040, -0.0177,  ...,  0.0284,  0.0151, -0.0408],\n",
      "          [-0.0074, -0.0150, -0.0406,  ..., -0.0350, -0.0421,  0.0040],\n",
      "          ...,\n",
      "          [-0.0144, -0.0137,  0.0043,  ..., -0.0058, -0.0158,  0.0223],\n",
      "          [-0.0256,  0.0263,  0.0159,  ...,  0.0426,  0.0161,  0.0261],\n",
      "          [ 0.0303, -0.0093, -0.0173,  ...,  0.0142,  0.0266,  0.0330]]],\n",
      "\n",
      "\n",
      "        [[[-0.0008, -0.0226,  0.0084,  ...,  0.0096, -0.0239, -0.0056],\n",
      "          [ 0.0109,  0.0153, -0.0091,  ..., -0.0061,  0.0308,  0.0136],\n",
      "          [-0.0191,  0.0367,  0.0058,  ..., -0.0259, -0.0105,  0.0133],\n",
      "          ...,\n",
      "          [-0.0261,  0.0176, -0.0010,  ..., -0.0320,  0.0294, -0.0182],\n",
      "          [-0.0398, -0.0355, -0.0415,  ...,  0.0409, -0.0237, -0.0269],\n",
      "          [-0.0123,  0.0121,  0.0259,  ..., -0.0206,  0.0192,  0.0143]],\n",
      "\n",
      "         [[-0.0069,  0.0141, -0.0409,  ...,  0.0141,  0.0269,  0.0145],\n",
      "          [ 0.0232,  0.0209,  0.0351,  ..., -0.0324, -0.0317,  0.0231],\n",
      "          [-0.0207,  0.0410, -0.0301,  ...,  0.0217,  0.0352,  0.0356],\n",
      "          ...,\n",
      "          [ 0.0312,  0.0097, -0.0314,  ...,  0.0412,  0.0307, -0.0407],\n",
      "          [ 0.0282, -0.0381,  0.0288,  ...,  0.0302,  0.0318, -0.0054],\n",
      "          [ 0.0204, -0.0261,  0.0149,  ..., -0.0130, -0.0114,  0.0165]],\n",
      "\n",
      "         [[-0.0032, -0.0152,  0.0018,  ...,  0.0012, -0.0213,  0.0005],\n",
      "          [-0.0110,  0.0180,  0.0127,  ...,  0.0375, -0.0298, -0.0224],\n",
      "          [-0.0180,  0.0405,  0.0044,  ...,  0.0259, -0.0339,  0.0235],\n",
      "          ...,\n",
      "          [ 0.0197,  0.0149,  0.0165,  ..., -0.0281, -0.0191,  0.0246],\n",
      "          [-0.0392,  0.0226, -0.0317,  ...,  0.0036, -0.0156, -0.0190],\n",
      "          [-0.0045,  0.0036, -0.0060,  ..., -0.0224,  0.0124,  0.0406]]],\n",
      "\n",
      "\n",
      "        [[[-0.0174,  0.0333,  0.0133,  ...,  0.0029,  0.0302,  0.0198],\n",
      "          [-0.0082, -0.0230,  0.0281,  ...,  0.0073, -0.0382, -0.0326],\n",
      "          [-0.0202,  0.0372,  0.0043,  ..., -0.0281,  0.0334,  0.0163],\n",
      "          ...,\n",
      "          [-0.0074, -0.0143, -0.0257,  ...,  0.0116,  0.0305, -0.0150],\n",
      "          [-0.0313, -0.0314,  0.0209,  ...,  0.0072,  0.0234,  0.0176],\n",
      "          [-0.0109, -0.0188,  0.0141,  ...,  0.0128, -0.0207, -0.0247]],\n",
      "\n",
      "         [[-0.0086, -0.0181, -0.0313,  ..., -0.0172, -0.0218, -0.0015],\n",
      "          [-0.0271,  0.0062,  0.0317,  ...,  0.0051,  0.0386, -0.0336],\n",
      "          [ 0.0092, -0.0287,  0.0064,  ...,  0.0335,  0.0102, -0.0047],\n",
      "          ...,\n",
      "          [ 0.0071, -0.0232,  0.0216,  ..., -0.0028,  0.0081,  0.0091],\n",
      "          [ 0.0249,  0.0046,  0.0121,  ...,  0.0078, -0.0245, -0.0330],\n",
      "          [ 0.0422, -0.0380,  0.0361,  ..., -0.0247, -0.0054, -0.0355]],\n",
      "\n",
      "         [[-0.0141,  0.0412,  0.0285,  ...,  0.0239,  0.0038,  0.0207],\n",
      "          [-0.0403,  0.0126,  0.0390,  ..., -0.0085, -0.0177, -0.0209],\n",
      "          [ 0.0099,  0.0148,  0.0067,  ..., -0.0162, -0.0244,  0.0380],\n",
      "          ...,\n",
      "          [ 0.0417, -0.0268, -0.0096,  ..., -0.0135,  0.0058,  0.0409],\n",
      "          [ 0.0098, -0.0222, -0.0396,  ...,  0.0083, -0.0077,  0.0338],\n",
      "          [ 0.0136, -0.0008,  0.0214,  ..., -0.0353,  0.0328, -0.0346]]]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0.1000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "nn.init.xavier_uniform_(conv1.weight)\n",
    "print(conv1.weight)\n",
    "nn.init.constant_(conv1.bias, 0.1)\n",
    "print(conv1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T18:18:55.024432Z",
     "start_time": "2020-03-24T18:18:55.020427Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "\n",
    "#define the initial function to init the layer's parameters for the network\n",
    "def weigth_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_uniform_(m.weight.data)\n",
    "        init.constant_(m.bias.data, 0.1)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(0, 0.01)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T18:19:28.668171Z",
     "start_time": "2020-03-24T18:19:28.665151Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.apply(weights_init) #apply函数会递归地搜索网络内的所有module并把参数表示的函数应用到所有的module上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
