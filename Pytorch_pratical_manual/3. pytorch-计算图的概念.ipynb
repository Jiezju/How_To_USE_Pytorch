{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 概念\n",
    "\n",
    "    计算图是描述计算的有向无环图\n",
    "    \n",
    "- 组成\n",
    "\n",
    "    计算图主要由节点（Node）和边（Edge）两个元素组成\n",
    "    \n",
    "**结点**\n",
    "- 表示数据张量\n",
    "\n",
    "**边**\n",
    "- 表示运算，如加减乘除卷积等操作\n",
    "\n",
    "**例**\n",
    "\n",
    "\n",
    "$$y=(w+x)*(w+1)$$\n",
    "\n",
    "- 分解\n",
    "\n",
    "$$a=w+x$$\n",
    "\n",
    "$$b=w+1$$\n",
    "\n",
    "$$y=a*b$$\n",
    "\n",
    "\n",
    "- 则对应计算图为：\n",
    "\n",
    "![](./img/jisuan.png)\n",
    "\n",
    "- 好处\n",
    "\n",
    "    方便计算与求导\n",
    "    \n",
    "    \n",
    "![](./img/daoshu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================  exmaple 1 ===============================\n",
    "# 通过pytorch构建计算图\n",
    "\n",
    "##初始化x和w，注意需要计算导数\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "##构建计算图\n",
    "one = torch.tensor([1.])\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, one)\n",
    "y = torch.mul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**张量的叶子结点is_leaf**\n",
    "    \n",
    "\n",
    "- 叶子结点的概念\n",
    "\n",
    "    用户创建的张量为结点张量，如上计算图中的$x$和$w$,is_leaf为True时，表示该张量为结点张量\n",
    "    \n",
    "    当反向传播求导数时，只会保存叶子结点的导数（$x$和$w$），非叶子结点（如$a和b$）不会被保存导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_isleaf:True\tw_isleaf:True\ta_isleaf:False\tb_isleaf:False\n",
      "x_grad:tensor([2.])\tw_grad:tensor([5.])\ta_grad:None\tb_grad:None\n"
     ]
    }
   ],
   "source": [
    "# ===============================  exmaple 2 ===============================\n",
    "# 通过pytorch构建反向传播求导数，默认不保存中间变量的梯度\n",
    "\n",
    "##反向传播计算\n",
    "y.backward()\n",
    "print(\"x_isleaf:{}\\tw_isleaf:{}\\ta_isleaf:{}\\tb_isleaf:{}\".format(\n",
    "    x.is_leaf, w.is_leaf, a.is_leaf, b.is_leaf))\n",
    "print(\"x_grad:{}\\tw_grad:{}\\ta_grad:{}\\tb_grad:{}\".format(\n",
    "    x.grad, w.grad, a.grad, b.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 注解\n",
    "\n",
    "    默认情况下，只有叶子结点的张量导数才会被保存，否则，中间导数不会被保存\n",
    "    \n",
    "    如果想保存中间变量的梯度，可以使用retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_isleaf:True\tw_isleaf:True\ta_isleaf:False\tb_isleaf:False\n",
      "x_grad:tensor([2.])\tw_grad:tensor([5.])\ta_grad:tensor([2.])\tb_grad:tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "# ===============================  exmaple 3 ===============================\n",
    "# 通过pytorch构建反向传播求导数，保存中间变量的梯度\n",
    "\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "a.retain_grad()  ##采用retain_grad()保存中间变量的梯度\n",
    "b = torch.add(w, 1)\n",
    "b.retain_grad()\n",
    "\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"x_isleaf:{}\\tw_isleaf:{}\\ta_isleaf:{}\\tb_isleaf:{}\".format(\n",
    "    x.is_leaf, w.is_leaf, a.is_leaf, b.is_leaf))\n",
    "print(\"x_grad:{}\\tw_grad:{}\\ta_grad:{}\\tb_grad:{}\".format(\n",
    "    x.grad, w.grad, a.grad, b.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grad_fn\n",
    "\n",
    "    tensor的grad_fn表示这个tensor的生成方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_fn:\n",
      " None None <AddBackward0 object at 0x00000000064A3860> <AddBackward0 object at 0x00000000064A36A0> <MulBackward0 object at 0x00000000064A3320>\n"
     ]
    }
   ],
   "source": [
    "# 查看 grad_fn\n",
    "print(\"grad_fn:\\n\", w.grad_fn, x.grad_fn, a.grad_fn, b.grad_fn, y.grad_fn)\n",
    "##注意由于w和x是用户创建的，因此没有运算方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动态计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/dongtai.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
