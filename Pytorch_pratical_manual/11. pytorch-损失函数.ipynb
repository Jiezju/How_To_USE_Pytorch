{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数的底层实现与改写方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T10:53:50.274692Z",
     "start_time": "2020-04-26T10:53:46.091079Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T11:03:44.282282Z",
     "start_time": "2020-04-26T11:03:44.279275Z"
    }
   },
   "outputs": [],
   "source": [
    "class _Loss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(_Loss, self).__init__()\n",
    "        if reduction is not None:\n",
    "            pass\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    nn.CrossEntropyloss(weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')\n",
    "\n",
    "- 功能\n",
    "\n",
    "    交叉熵计算 包含了SoftMax计算\n",
    "    \n",
    "- weight\n",
    "\n",
    "    各类别的loss设置权值\n",
    "\n",
    "- ignore_index\n",
    "\n",
    "    忽略某个类别\n",
    "\n",
    "- reduction\n",
    "\n",
    "   计算模式：None/sum/mean \n",
    "   - None 逐个元素计算\n",
    "   - sum  所有元素求和，返回标量\n",
    "   - mean 加权平均\n",
    "   \n",
    "- 计算公式\n",
    "\n",
    "    $$loss(x,class)=weight[class]*(-x[class]+log{\\sum_jexp(x[j])})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:55:36.749030Z",
     "start_time": "2020-04-26T13:55:36.742019Z"
    }
   },
   "outputs": [],
   "source": [
    "# fake data  inputs为未归一化的输出向量\n",
    "inputs = torch.tensor([[1, 2], [1, 3], [1, 3]], dtype=torch.float)\n",
    "target = torch.tensor([0, 1, 1], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T11:03:08.553003Z",
     "start_time": "2020-04-26T11:03:08.373989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.]\n",
      "[[1. 2.]\n",
      " [1. 3.]\n",
      " [1. 3.]]\n",
      "第一个样本loss为:  1.3132617\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "公式实现nn.CrossEntropyloss()\n",
    "'''\n",
    "\n",
    "idx = 0\n",
    "\n",
    "input_1 = inputs.detach().numpy()[idx]  # [1, 2]\n",
    "print(input_1)\n",
    "print(inputs.numpy())\n",
    "target_1 = target.numpy()[idx]  # [0]\n",
    "\n",
    "# 第一项\n",
    "x_class = input_1[target_1]\n",
    "\n",
    "# 第二项\n",
    "sigma_exp_x = np.sum(list(map(np.exp, input_1)))\n",
    "#map(np.exp, input_1)  对input_1的每个元素计算exp()函数\n",
    "log_sigma_exp_x = np.log(sigma_exp_x)\n",
    "\n",
    "# 输出loss\n",
    "loss_1 = -x_class + log_sigma_exp_x\n",
    "\n",
    "print(\"第一个样本loss为: \", loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T11:03:09.784501Z",
     "start_time": "2020-04-26T11:03:09.242363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss:\n",
      "  tensor([1.3133, 0.1269, 0.1269]) tensor(1.5671) tensor(0.5224)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------- CrossEntropy loss: reduction -----------------------------------\n",
    "\n",
    "# def loss function\n",
    "loss_f_none = nn.CrossEntropyLoss(weight=None, reduction='none')  ##返回向量 分别计算每个样本的损失\n",
    "loss_f_sum = nn.CrossEntropyLoss(weight=None, reduction='sum')  ##返回损失和 计算所有样本损失和\n",
    "loss_f_mean = nn.CrossEntropyLoss(weight=None, reduction='mean')  ##返回损失均值  计算所有样本的损失均值\n",
    "\n",
    "# forward\n",
    "loss_none = loss_f_none(inputs, target)\n",
    "loss_sum = loss_f_sum(inputs, target)\n",
    "loss_mean = loss_f_mean(inputs, target)\n",
    "\n",
    "# view\n",
    "print(\"Cross Entropy Loss:\\n \", loss_none, loss_sum, loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T11:03:12.192578Z",
     "start_time": "2020-04-26T11:03:12.174620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights:  tensor([1., 2.])\n",
      "tensor([1.3133, 0.2539, 0.2539]) tensor(1.8210) tensor(0.3642)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------- weight -----------------------------------\n",
    "\n",
    "# def loss function\n",
    "weights = torch.tensor([1, 2], dtype=torch.float)  #0类别损失乘以1,1类别损失乘以2\n",
    "# weights = torch.tensor([0.7, 0.3], dtype=torch.float)\n",
    "\n",
    "loss_f_none_w = nn.CrossEntropyLoss(weight=weights, reduction='none')\n",
    "loss_f_sum = nn.CrossEntropyLoss(weight=weights, reduction='sum')\n",
    "loss_f_mean = nn.CrossEntropyLoss(weight=weights, reduction='mean')\n",
    "\n",
    "# forward\n",
    "loss_none_w = loss_f_none_w(inputs, target)\n",
    "loss_sum = loss_f_sum(inputs, target)\n",
    "loss_mean = loss_f_mean(inputs, target)\n",
    "\n",
    "# view\n",
    "print(\"\\nweights: \", weights)\n",
    "print(loss_none_w, loss_sum, loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3641947731375694\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------- compute by hand\n",
    "\n",
    "# flag = 1\n",
    "weights = torch.tensor([1, 2], dtype=torch.float)\n",
    "weights_all = np.sum(list(map(lambda x: weights.numpy()[x], target.numpy())))  # [0, 1, 1]  # [1 2 2]\n",
    "\n",
    "mean = 0\n",
    "loss_sep = loss_none.detach().numpy()\n",
    "for i in range(target.shape[0]):\n",
    "\n",
    "    x_class = target.numpy()[i]\n",
    "    tmp = loss_sep[i] * (weights.numpy()[x_class] / weights_all)\n",
    "    mean += tmp\n",
    "\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    nn.NLLLoss(weight=None,ignore_index=-100,reduction='mean')\n",
    "\n",
    "- 功能\n",
    "\n",
    "    实现负对数似然的负号功能\n",
    "    \n",
    "- weight\n",
    "\n",
    "    各类别的loss设置权值\n",
    "\n",
    "- ignore_index\n",
    "\n",
    "    忽略某个类别\n",
    "\n",
    "- reduction\n",
    "\n",
    "   计算模式：None/sum/mean \n",
    "   - None 逐个元素计算\n",
    "   - sum  所有元素求和，返回标量\n",
    "   - mean 加权平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights:  tensor([1., 1.])\n",
      "NLL Loss tensor([-1., -3., -3.]) tensor(-7.) tensor(-2.3333)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------- 2 NLLLoss -----------------------------------\n",
    "\n",
    "weights = torch.tensor([1, 1], dtype=torch.float)\n",
    "\n",
    "loss_f_none_w = nn.NLLLoss(weight=weights, reduction='none')\n",
    "loss_f_sum = nn.NLLLoss(weight=weights, reduction='sum')\n",
    "loss_f_mean = nn.NLLLoss(weight=weights, reduction='mean')\n",
    "\n",
    "# forward\n",
    "loss_none_w = loss_f_none_w(inputs, target)\n",
    "loss_sum = loss_f_sum(inputs, target)\n",
    "loss_mean = loss_f_mean(inputs, target)\n",
    "\n",
    "# view\n",
    "print(\"\\nweights: \", weights)\n",
    "print(\"NLL Loss\", loss_none_w, loss_sum, loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    nn.BCELoss(weight=None,reduction='mean')\n",
    "\n",
    "- 功能\n",
    "\n",
    "    二分类交叉熵\n",
    "    \n",
    "- 计算公式\n",
    "\n",
    "    $$l_n=-w_n*(y_n*log(x_n)+(1-y_n)*log(1-x_n))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE inputs:  tensor([[1., 2.],\n",
      "        [2., 2.],\n",
      "        [3., 4.],\n",
      "        [4., 5.]])\n",
      "第一个loss为:  -0.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "函数实现\n",
    "'''\n",
    "\n",
    "inputs = torch.tensor([[1, 2], [2, 2], [3, 4], [4, 5]], dtype=torch.float)\n",
    "target = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]], dtype=torch.float)\n",
    "\n",
    "idx = 0\n",
    "\n",
    "x_i = inputs.detach().numpy()[idx, idx]\n",
    "y_i = target.numpy()[idx, idx]              #\n",
    "\n",
    "# loss\n",
    "# l_i = -[ y_i * np.log(x_i) + (1-y_i) * np.log(1-y_i) ]      # np.log(0) = nan\n",
    "l_i = -y_i * np.log(x_i) if y_i else -(1-y_i) * np.log(1-x_i)\n",
    "\n",
    "# 输出loss\n",
    "print(\"BCE inputs: \", inputs)\n",
    "print(\"第一个loss为: \", l_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights:  tensor([1., 1.])\n",
      "BCE Loss tensor([[0.3133, 2.1269],\n",
      "        [0.1269, 2.1269],\n",
      "        [3.0486, 0.0181],\n",
      "        [4.0181, 0.0067]]) tensor(11.7856) tensor(1.4732)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2], [2, 2], [3, 4], [4, 5]], dtype=torch.float)\n",
    "target = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]], dtype=torch.float)\n",
    "\n",
    "target_bce = target\n",
    "\n",
    "# itarget\n",
    "inputs = torch.sigmoid(inputs)  ##输入值在0-1之间\n",
    "\n",
    "weights = torch.tensor([1, 1], dtype=torch.float)\n",
    "\n",
    "loss_f_none_w = nn.BCELoss(weight=weights, reduction='none')\n",
    "loss_f_sum = nn.BCELoss(weight=weights, reduction='sum')\n",
    "loss_f_mean = nn.BCELoss(weight=weights, reduction='mean')\n",
    "\n",
    "# forward\n",
    "loss_none_w = loss_f_none_w(inputs, target_bce)\n",
    "loss_sum = loss_f_sum(inputs, target_bce)\n",
    "loss_mean = loss_f_mean(inputs, target_bce)\n",
    "\n",
    "# view\n",
    "print(\"\\nweights: \", weights)\n",
    "print(\"BCE Loss\", loss_none_w, loss_sum, loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    nn.BCEWithLogitsLoss(self, weight=None, reduction='mean', pos_weight=None)\n",
    "\n",
    "- 功能\n",
    "\n",
    "    结合sigmoid函数与二分类的交叉熵\n",
    "\n",
    "- pos_weight\n",
    "\n",
    "    正样本的权值\n",
    "    \n",
    "- weight\n",
    "\n",
    "    各类别的loss设置权值\n",
    "\n",
    "- ignore_index\n",
    "\n",
    "    忽略某个类别\n",
    "\n",
    "- reduction\n",
    "\n",
    "   计算模式：None/sum/mean \n",
    "   - None 逐个元素计算\n",
    "   - sum  所有元素求和，返回标量\n",
    "   - mean 加权平均\n",
    "   \n",
    "- 计算公式\n",
    "\n",
    "     $$l_n=-w_n*(y_n*log(\\sigma(x_n))+(1-y_n)*log(1-\\sigma(x_n)))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights:  tensor([1., 1.])\n",
      "tensor([[0.3133, 2.1269],\n",
      "        [0.1269, 2.1269],\n",
      "        [3.0486, 0.0181],\n",
      "        [4.0181, 0.0067]]) tensor(11.7856) tensor(1.4732)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------- 4 BCE with Logis Loss -----------------------------------\n",
    "\n",
    "inputs = torch.tensor([[1, 2], [2, 2], [3, 4], [4, 5]], dtype=torch.float)\n",
    "target = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]], dtype=torch.float)\n",
    "\n",
    "target_bce = target\n",
    "\n",
    "# inputs = torch.sigmoid(inputs)  调用BCEWithLogitsLoss()函数不需要再添加Sigmoid函数\n",
    "\n",
    "weights = torch.tensor([1, 1], dtype=torch.float)\n",
    "\n",
    "loss_f_none_w = nn.BCEWithLogitsLoss(weight=weights, reduction='none')\n",
    "loss_f_sum = nn.BCEWithLogitsLoss(weight=weights, reduction='sum')\n",
    "loss_f_mean = nn.BCEWithLogitsLoss(weight=weights, reduction='mean')\n",
    "\n",
    "# forward\n",
    "loss_none_w = loss_f_none_w(inputs, target_bce)\n",
    "loss_sum = loss_f_sum(inputs, target_bce)\n",
    "loss_mean = loss_f_mean(inputs, target_bce)\n",
    "\n",
    "# view\n",
    "print(\"\\nweights: \", weights)\n",
    "print(loss_none_w, loss_sum, loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pos_weights:  tensor([3.])\n",
      "tensor([[0.9398, 2.1269],\n",
      "        [0.3808, 2.1269],\n",
      "        [3.0486, 0.0544],\n",
      "        [4.0181, 0.0201]]) tensor(12.7158) tensor(1.5895)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------- pos weight\n",
    "\n",
    "inputs = torch.tensor([[1, 2], [2, 2], [3, 4], [4, 5]], dtype=torch.float)\n",
    "target = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]], dtype=torch.float)\n",
    "\n",
    "target_bce = target\n",
    "\n",
    "# itarget\n",
    "# inputs = torch.sigmoid(inputs)\n",
    "\n",
    "weights = torch.tensor([1], dtype=torch.float)\n",
    "pos_w = torch.tensor([3], dtype=torch.float)  # 3\n",
    "\n",
    "loss_f_none_w = nn.BCEWithLogitsLoss(weight=weights,\n",
    "                                     reduction='none',\n",
    "                                     pos_weight=pos_w)\n",
    "loss_f_sum = nn.BCEWithLogitsLoss(weight=weights,\n",
    "                                  reduction='sum',\n",
    "                                  pos_weight=pos_w)\n",
    "loss_f_mean = nn.BCEWithLogitsLoss(weight=weights,\n",
    "                                   reduction='mean',\n",
    "                                   pos_weight=pos_w)\n",
    "\n",
    "# forward\n",
    "loss_none_w = loss_f_none_w(inputs, target_bce)\n",
    "loss_sum = loss_f_sum(inputs, target_bce)\n",
    "loss_mean = loss_f_mean(inputs, target_bce)\n",
    "\n",
    "# view\n",
    "print(\"\\npos_weights: \", pos_w)\n",
    "print(loss_none_w, loss_sum, loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "- 计算公式：\n",
    "\n",
    "    $$l_n=|x_n-y_n|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "target:tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "L1 loss:tensor([[2., 2.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------- 5 L1 loss ----------------------------------------------\n",
    "\n",
    "inputs = torch.ones((2, 2))\n",
    "target = torch.ones((2, 2)) * 3\n",
    "\n",
    "loss_f = nn.L1Loss(reduction='none')\n",
    "loss = loss_f(inputs, target)\n",
    "\n",
    "print(\"input:{}\\ntarget:{}\\nL1 loss:{}\".format(inputs, target, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "    $$l_n=(x_n-y_n)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss:tensor([[4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------- 6 MSE loss ----------------------------------------------\n",
    "\n",
    "loss_f_mse = nn.MSELoss(reduction='none')\n",
    "loss_mse = loss_f_mse(inputs, target)\n",
    "\n",
    "print(\"MSE loss:{}\".format(loss_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "    $$\n",
    "z_i=\n",
    "\\begin{cases}\n",
    "0.5*(x_i-y_i)^2, &if\\ |x_i-y_i|<1\\\\\n",
    "|x_i-y_i|-0.5, &otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$loss(x,y)=\\frac 1n\\sum_i{z_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEQCAYAAABSlhj/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhiDdQbGGGOwzsAYYwzw/0xqe1MxNbifAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xef30048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------------------- 7 Smooth L1 loss ----------------------------------------------\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "inputs = torch.linspace(-3, 3, steps=500)\n",
    "target = torch.zeros_like(inputs)\n",
    "\n",
    "loss_f = nn.SmoothL1Loss(reduction='none')\n",
    "\n",
    "loss_smooth = loss_f(inputs, target)\n",
    "\n",
    "loss_l1 = np.abs(inputs.numpy())\n",
    "\n",
    "plt.plot(inputs.numpy(), loss_smooth.numpy(), label='Smooth L1 Loss')\n",
    "plt.plot(inputs.numpy(), loss_l1, label='L1 loss')\n",
    "plt.xlabel('x_i - y_i')\n",
    "plt.ylabel('loss value')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.PoissonNLLLoss(log_input=True, full=False,  eps=1e-08,  reduction='mean')\n",
    "- 功能\n",
    "\n",
    "    泊松分布的负对数似然损失函数\n",
    "\n",
    "- log_input\n",
    "\n",
    "    输入是否为对数形式\n",
    "    \n",
    "- full\n",
    "    \n",
    "    计算所有loss，一般不改变\n",
    "\n",
    "- eps\n",
    "\n",
    "    防止log为NAN\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "    log_input=True:\n",
    "    \n",
    "    $loss(input,target)=exp(input) - target * input$ \n",
    "    \n",
    "    log_input=False:\n",
    "    \n",
    "    $loss(input,target)=input - target * log(input+eps)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个元素loss: tensor(-0.1584)\n",
      "input:tensor([[-1.5419, -1.2423],\n",
      "        [-0.3392, -1.5404]])\n",
      "target:tensor([[-0.2415,  1.2992],\n",
      "        [-0.3380,  0.6951]])\n",
      "Poisson NLL loss:tensor([[-0.1584,  1.9027],\n",
      "        [ 0.5977,  1.2850]])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------- 8 Poisson NLL Loss ----------------------------------------------\n",
    "\n",
    "inputs = torch.randn((2, 2))\n",
    "target = torch.randn((2, 2))\n",
    "\n",
    "##手动实现\n",
    "idx = 0\n",
    "\n",
    "loss_1 = torch.exp(inputs[idx, idx]) - target[idx, idx] * inputs[idx, idx]\n",
    "\n",
    "print(\"第一个元素loss:\", loss_1)\n",
    "\n",
    "##pytorch框架实现\n",
    "loss_f = nn.PoissonNLLLoss(log_input=True, full=False, reduction='none')\n",
    "loss = loss_f(inputs, target)\n",
    "print(\"input:{}\\ntarget:{}\\nPoisson NLL loss:{}\".format(inputs, target, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.KLDivLoss(reduction='mean')\n",
    "\n",
    "- 功能\n",
    "\n",
    "    计算 input 和 target 之间的 KL 散度\n",
    "    \n",
    "- 计算公式\n",
    "\n",
    "    需要要提前对输入取nn.logsoftmax($x_n$)\n",
    "    \n",
    "    $$l_n=y_n*(logy_n-x_n)$$\n",
    "\n",
    "- reduction\n",
    "\n",
    "    注意reduction中的batchmean，表示以batchsize为总量求取均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个元素loss: tensor(-0.5448)\n",
      "loss_none:\n",
      "tensor([[-0.5448, -0.1648, -0.1598],\n",
      "        [-0.2503, -0.4597, -0.4219]])\n",
      "loss_mean:\n",
      "-0.3335360288619995\n",
      "loss_bs_mean:\n",
      "-1.0006080865859985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1932: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------- 9 KL Divergence Loss ----------------\n",
    "\n",
    "inputs = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.3, 0.5]])\n",
    "inputs_log = torch.log(inputs)  ##log输入\n",
    "target = torch.tensor([[0.9, 0.05, 0.05], [0.1, 0.7, 0.2]], dtype=torch.float)\n",
    "\n",
    "#手动实现\n",
    "idx = 0\n",
    "loss_1 = target[idx, idx] * (torch.log(target[idx, idx]) - inputs[idx, idx])\n",
    "print(\"第一个元素loss:\", loss_1)\n",
    "\n",
    "#pytorch框架\n",
    "loss_f_none = nn.KLDivLoss(reduction='none')\n",
    "loss_f_mean = nn.KLDivLoss(reduction='mean')\n",
    "loss_f_bs_mean = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "loss_none = loss_f_none(inputs, target)\n",
    "loss_mean = loss_f_mean(inputs, target)\n",
    "loss_bs_mean = loss_f_bs_mean(inputs, target)\n",
    "\n",
    "print(\"loss_none:\\n{}\\nloss_mean:\\n{}\\nloss_bs_mean:\\n{}\".format(\n",
    "    loss_none, loss_mean, loss_bs_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.MarginRankingLoss(margin=0.0, reduction='mean')\n",
    " \n",
    "- 功能\n",
    "\n",
    "    比较两个向量的相似度，然后进行排序\n",
    "\n",
    "- margin\n",
    "\n",
    "    边界值，x1和x2的差异值\n",
    "    \n",
    "- 计算公式\n",
    "\n",
    "    $$loss(x,y)=max(0,-y*(x_1-x_2)+margin)$$\n",
    "    \n",
    "    - 公式分析：\n",
    "    - y=1时，希望$x_1$比$x_2$大，$x_1>x_2$,loss为0\n",
    "    - y=-1时，希望$x_1$比$x_2$小，$x_1<x_2$,loss为0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------- 10 Margin Ranking Loss -----------------------------------------\n",
    "\n",
    "x1 = torch.tensor([[1], [2], [3]], dtype=torch.float)\n",
    "x2 = torch.tensor([[2], [2], [2]], dtype=torch.float)\n",
    "\n",
    "target = torch.tensor([1, 1, -1], dtype=torch.float)\n",
    "\n",
    "loss_f_none = nn.MarginRankingLoss(margin=0, reduction='none')\n",
    "\n",
    "loss = loss_f_none(x1, x2, target)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.MultiLabelMarginLoss(reduction='mean')\n",
    "\n",
    "- 功能\n",
    "\n",
    "    多标签边界损失函数\n",
    "    \n",
    "- 计算公式\n",
    "\n",
    "    $$loss(x,y)=\\sum{_i}{_j}\\frac {max(0,1-(x[y[j]]-x[i]))}{x.size(0)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8500])\n",
      "tensor([0.1000, 0.2000, 0.4000, 0.8000])\n",
      "tensor(0.8500)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------- 11 Multi Label Margin Loss ---------------------\n",
    "\n",
    "x = torch.tensor([[0.1, 0.2, 0.4, 0.8]])\n",
    "y = torch.tensor([[0, 3, -1, -1]], dtype=torch.long)\n",
    "\n",
    "##pytorch框架\n",
    "loss_f = nn.MultiLabelMarginLoss(reduction='none')\n",
    "loss = loss_f(x, y)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "##公式实现\n",
    "\n",
    "x = x[0]\n",
    "print(x)\n",
    "item_1 = (1 - (x[0] - x[1])) + (1 - (x[0] - x[2]))  # [0]\n",
    "item_2 = (1 - (x[3] - x[1])) + (1 - (x[3] - x[2]))  # [3]\n",
    "loss_h = (item_1 + item_2) / x.shape[0]\n",
    "print(loss_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.SoftMarginLoss(reduction='mean')\n",
    "\n",
    "- 功能\n",
    "    \n",
    "    计算二分类的逻辑斯特损失\n",
    "    \n",
    "- 计算公式\n",
    "\n",
    "    $$loss(x,y)=\\sum_i \\frac {log(1+exp(-y[i]*x[i])}{x.nelement()})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8544)\n",
      "SoftMargin:  tensor([[0.8544, 0.4032],\n",
      "        [0.4741, 0.9741]])\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------- 12 SoftMargin Loss -----------------------------------------\n",
    "\n",
    "inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]])\n",
    "target = torch.tensor([[-1, 1], [1, -1]], dtype=torch.float)\n",
    "\n",
    "\n",
    "#手动实现\n",
    "idx = 0\n",
    "inputs_i = inputs[idx, idx]\n",
    "target_i = target[idx, idx]\n",
    "loss_h = np.log(1 + np.exp(-target_i * inputs_i))\n",
    "print(loss_h)\n",
    "\n",
    "##pytorch框架\n",
    "loss_f = nn.SoftMarginLoss(reduction='none')\n",
    "loss = loss_f(inputs, target)\n",
    "print(\"SoftMargin: \", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.MultiLabelSoftMarginLoss(weight=None, reduction='mean')\n",
    "    \n",
    "- 功能\n",
    "\n",
    "    SoftMarginLoss的多标签版本\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "    $$loss(x,y)=-\\frac 1C \\sum_i y[i]*log(1+exp(-x[i])^-1)+(1-y[i])*log(\\frac {exp(-x[i])}{1+exp(-x[i])})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5429)\n",
      "MultiLabel SoftMargin:  tensor([0.5429])\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------- 13 MultiLabel SoftMargin Loss ---------------------------------\n",
    "\n",
    "##手动实现\n",
    "inputs = torch.tensor([[0.3, 0.7, 0.8]])\n",
    "target = torch.tensor([[0, 1, 1]], dtype=torch.float)\n",
    "\n",
    "i_0 = torch.log(torch.exp(-inputs[0, 0]) / (1 + torch.exp(-inputs[0, 0])))\n",
    "i_1 = torch.log(1 / (1 + torch.exp(-inputs[0, 1])))\n",
    "i_2 = torch.log(1 / (1 + torch.exp(-inputs[0, 2])))\n",
    "loss_h = (i_0 + i_1 + i_2) / -3\n",
    "print(loss_h)\n",
    "\n",
    "#pytorch 框架\n",
    "loss_f = nn.MultiLabelSoftMarginLoss(reduction='none')\n",
    "loss = loss_f(inputs, target)\n",
    "print(\"MultiLabel SoftMargin: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None,  reduction='mean')\n",
    "\n",
    "- 功能\n",
    "\n",
    "    计算多分类的合页损失\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "    $$loss(x,y)=\\frac {\\sum_i max(0,w[y]*(margin-x[y]+x[i])^p)}{x.size(0)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi Margin Loss:  tensor([0.8000, 0.7000])\n",
      "tensor(0.8000)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------- 14 Multi Margin Loss -----------------------------------------\n",
    "\n",
    "x = torch.tensor([[0.1, 0.2, 0.7], [0.2, 0.5, 0.3]])\n",
    "y = torch.tensor([1, 2], dtype=torch.long)\n",
    "\n",
    "loss_f = nn.MultiMarginLoss(reduction='none')\n",
    "\n",
    "loss = loss_f(x, y)\n",
    "\n",
    "print(\"Multi Margin Loss: \", loss)\n",
    "\n",
    "# --------------------------------- 手动计算\n",
    "\n",
    "x = x[0]\n",
    "margin = 1\n",
    "i_0 = margin - (x[1] - x[0])\n",
    "# i_1 = margin - (x[1] - x[1])\n",
    "i_2 = margin - (x[1] - x[2])\n",
    "\n",
    "loss_h = (i_0 + i_2) / x.shape[0]\n",
    "\n",
    "print(loss_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, reduction='mean')\n",
    "\n",
    "- 功能\n",
    "\n",
    "    计算三元组损失，常用于人脸验证\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "    $$L(a,p,n)=max\\{d(a_i,p_i)-d(a_i,n_i)+margin,0\\}$$\n",
    "    \n",
    "    $$d(x_i,y_i)=|x_i-y_i|_p$$\n",
    "    \n",
    "![](./img/trip.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Margin Loss tensor(1.5000)\n",
      "tensor([1.5000])\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------- 15 Triplet Margin Loss -----------------------------------------\n",
    "\n",
    "anchor = torch.tensor([[1.]])\n",
    "pos = torch.tensor([[2.]])\n",
    "neg = torch.tensor([[0.5]])\n",
    "\n",
    "loss_f = nn.TripletMarginLoss(margin=1.0, p=1)\n",
    "\n",
    "loss = loss_f(anchor, pos, neg)\n",
    "\n",
    "print(\"Triplet Margin Loss\", loss)\n",
    "\n",
    "# --------------------------------- compute by hand\n",
    "margin = 1\n",
    "a, p, n = anchor[0], pos[0], neg[0]\n",
    "\n",
    "d_ap = torch.abs(a-p)\n",
    "d_an = torch.abs(a-n)\n",
    "loss = d_ap - d_an + margin\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.HingeEmbeddingLoss(margin=1.0,  reduction='mean')\n",
    "\n",
    "- 功能\n",
    "    \n",
    "    计算两个输入的相似性，常用语非线性embedding和半监督学习\n",
    "    \n",
    "    注意：\n",
    "    输入x为两个输入之差的绝对值\n",
    "    \n",
    "$$\n",
    "l_n=\n",
    "\\begin{cases}\n",
    "x_n, &if\\ y_n = 1\\\\\n",
    "max(0,margin-x_n), &if\\  y_n = -1\n",
    "\\end{cases}\n",
    "$$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinge Embedding Loss tensor([[1.0000, 0.8000, 0.5000]])\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------- 16 Hinge Embedding Loss -----------------------------------------\n",
    "\n",
    "inputs = torch.tensor([[1., 0.8, 0.5]])\n",
    "target = torch.tensor([[1, 1, -1]])\n",
    "\n",
    "loss_f = nn.HingeEmbeddingLoss(margin=1, reduction='none')\n",
    "\n",
    "loss = loss_f(inputs, target)\n",
    "\n",
    "print(\"Hinge Embedding Loss\", loss)\n",
    "\n",
    "# --------------------------------- compute by hand\n",
    "\n",
    "margin = 1.\n",
    "loss = max(0, margin - inputs.numpy()[0, 2])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.CosineEmbeddingLoss(margin=0.0, reduction='mean')\n",
    "\n",
    "- 功能\n",
    "\n",
    "    采用余弦相似度计算两个输入的相似性\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "    $$\n",
    "loss(x,y)=\n",
    "\\begin{cases}\n",
    "1-cos(x_1,x_2), &if\\ y = 1\\\\\n",
    "max(0,cos(x_1,x_2)-margin), &if\\  y = -1\n",
    "\\end{cases}\n",
    "$$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Embedding Loss tensor([[0.0167, 0.9833]])\n",
      "0.016662120819091797 0.9833378791809082\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------- 17 Cosine Embedding Loss -----------------------------------------\n",
    "\n",
    "x1 = torch.tensor([[0.3, 0.5, 0.7], [0.3, 0.5, 0.7]])\n",
    "x2 = torch.tensor([[0.1, 0.3, 0.5], [0.1, 0.3, 0.5]])\n",
    "\n",
    "target = torch.tensor([[1, -1]], dtype=torch.float)\n",
    "\n",
    "loss_f = nn.CosineEmbeddingLoss(margin=0., reduction='none')\n",
    "\n",
    "loss = loss_f(x1, x2, target)\n",
    "\n",
    "print(\"Cosine Embedding Loss\", loss)\n",
    "\n",
    "# --------------------------------- compute by hand\n",
    "\n",
    "margin = 0.\n",
    "\n",
    "def cosine(a, b):\n",
    "    numerator = torch.dot(a, b)\n",
    "    denominator = torch.norm(a, 2) * torch.norm(b, 2)\n",
    "    return float(numerator/denominator)\n",
    "\n",
    "l_1 = 1 - (cosine(x1[0], x2[0]))\n",
    "\n",
    "l_2 = max(0, cosine(x1[0], x2[0]))\n",
    "print(l_1, l_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.nn.CTCLoss(blank=0, reduction='mean',zero_infinity=False)\n",
    "\n",
    "- 功能\n",
    "\n",
    "    解决时序数据的分类\n",
    "    \n",
    "- blank\n",
    "\n",
    "    标签\n",
    "\n",
    "- zero_infinity\n",
    "\n",
    "    无穷大的值或梯度置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC loss:  tensor(6.6590, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------- 18 CTC Loss -----------------------------------------\n",
    "\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "S = 30      # Target sequence length of longest target in batch\n",
    "S_min = 10  # Minimum target length, for demonstration purposes\n",
    "\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "inputs = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n",
    "\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(inputs, target, input_lengths, target_lengths)\n",
    "\n",
    "print(\"CTC loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
