{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:12:17.847184Z",
     "start_time": "2020-05-07T14:12:17.197181Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:13:54.731852Z",
     "start_time": "2020-05-07T14:13:54.726866Z"
    }
   },
   "source": [
    "**模型保存函数**\n",
    "\n",
    "    torch.save(obj,f)\n",
    "\n",
    "- obj\n",
    "\n",
    "    对象(可以是模型，张量，参数)\n",
    "\n",
    "- f\n",
    "\n",
    "    输出路径\n",
    "    \n",
    "**使用1：保存整个模型及参数**\n",
    "\n",
    "    torch.save(net,path)\n",
    "    \n",
    "**使用2：保存模型参数**\n",
    "\n",
    "    state_dict=net.state_dict()\n",
    "    torch.save(state_dict,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:24:02.718947Z",
     "start_time": "2020-05-07T14:24:02.711964Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "模型\n",
    "'''\n",
    "\n",
    "\n",
    "class LeNet2(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(LeNet2, self).__init__()\n",
    "        self.features = nn.Sequential(nn.Conv2d(3, 6, 5), nn.ReLU(),\n",
    "                                      nn.MaxPool2d(2, 2), nn.Conv2d(6, 16, 5),\n",
    "                                      nn.ReLU(), nn.MaxPool2d(2, 2))\n",
    "        self.classifier = nn.Sequential(nn.Linear(16 * 5 * 5, 120), nn.ReLU(),\n",
    "                                        nn.Linear(120, 84), nn.ReLU(),\n",
    "                                        nn.Linear(84, classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for p in self.parameters():\n",
    "            p.data.fill_(20191104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:24:39.123175Z",
     "start_time": "2020-05-07T14:24:38.794461Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练前:  tensor([[[ 0.0244,  0.0320,  0.0236,  0.0429, -0.0642],\n",
      "         [-0.0121,  0.0216,  0.0315,  0.0691, -0.1069],\n",
      "         [ 0.0681,  0.0427,  0.0456, -0.0759, -0.0435],\n",
      "         [ 0.0251,  0.0125,  0.0396, -0.0089, -0.0837],\n",
      "         [ 0.0997,  0.0329, -0.0125,  0.0471, -0.0490]],\n",
      "\n",
      "        [[-0.1042, -0.0166,  0.0783, -0.0810, -0.0347],\n",
      "         [-0.1133,  0.1128,  0.0365,  0.0527, -0.1127],\n",
      "         [-0.0754, -0.0348,  0.1081,  0.0082, -0.0328],\n",
      "         [ 0.0291, -0.0005, -0.0295, -0.1043,  0.0293],\n",
      "         [ 0.0808,  0.0860, -0.0920,  0.0826,  0.0746]],\n",
      "\n",
      "        [[ 0.0645,  0.0414,  0.0894,  0.0812, -0.1095],\n",
      "         [-0.0164, -0.0709, -0.1120, -0.0308, -0.0463],\n",
      "         [ 0.0895, -0.0027, -0.0596, -0.0174,  0.1027],\n",
      "         [ 0.1024,  0.1076,  0.0820, -0.0939, -0.0687],\n",
      "         [ 0.0550, -0.0546,  0.1137,  0.0184, -0.0103]]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "训练后:  tensor([[[20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.]],\n",
      "\n",
      "        [[20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.]],\n",
      "\n",
      "        [[20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.]]],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type LeNet2. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "net = LeNet2(classes=2019)\n",
    "\n",
    "# \"训练\"\n",
    "print(\"训练前: \", net.features[0].weight[0, ...])\n",
    "net.initialize()\n",
    "print(\"训练后: \", net.features[0].weight[0, ...])\n",
    "\n",
    "'''\n",
    "模型保存方法\n",
    "'''\n",
    "\n",
    "# 设置路径\n",
    "path_model = \"./model.pkl\" \n",
    "path_state_dict = \"./model_state_dict.pkl\"\n",
    "\n",
    "# 保存整个模型\n",
    "torch.save(net, path_model) \n",
    "\n",
    "# 保存模型参数\n",
    "net_state_dict = net.state_dict()\n",
    "torch.save(net_state_dict, path_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:16:25.267887Z",
     "start_time": "2020-05-07T14:16:25.263905Z"
    }
   },
   "source": [
    "**模型加载函数**\n",
    "\n",
    "    torch.load(f,map_location)\n",
    "\n",
    "- f\n",
    "\n",
    "    文件路径\n",
    "    \n",
    "- 指定存放位置\n",
    "\n",
    "    cpu gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:29:28.845582Z",
     "start_time": "2020-05-07T14:29:28.731182Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=84, out_features=2019, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ================================== load net ===========================\n",
    "'''\n",
    "整个模型的加载\n",
    "'''\n",
    "path_model = \"./model.pkl\"\n",
    "net_load = torch.load(path_model)\n",
    "\n",
    "print(net_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:34:09.027740Z",
     "start_time": "2020-05-07T14:34:09.019762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2019])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "使用加载的模型\n",
    "'''\n",
    "x = torch.randn((1, 3, 32, 32))\n",
    "out = net_load(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:34:23.596023Z",
     "start_time": "2020-05-07T14:34:23.587042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['features.0.weight', 'features.0.bias', 'features.3.weight', 'features.3.bias', 'classifier.0.weight', 'classifier.0.bias', 'classifier.2.weight', 'classifier.2.bias', 'classifier.4.weight', 'classifier.4.bias'])\n"
     ]
    }
   ],
   "source": [
    "# ================================== load state_dict ===========================\n",
    "\n",
    "\n",
    "path_state_dict = \"./model_state_dict.pkl\"\n",
    "state_dict_load = torch.load(path_state_dict)\n",
    "\n",
    "print(state_dict_load.keys()) # 打印参数字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:34:40.202356Z",
     "start_time": "2020-05-07T14:34:40.175429Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载前:  tensor([[[-0.0671,  0.0104,  0.0223,  0.0123,  0.0489],\n",
      "         [ 0.0446,  0.0674, -0.0812,  0.0379, -0.0725],\n",
      "         [ 0.0326, -0.0005, -0.0052,  0.0763,  0.0650],\n",
      "         [ 0.0428,  0.0140,  0.0897, -0.1132,  0.0294],\n",
      "         [-0.0729,  0.0889,  0.1053,  0.0051, -0.0066]],\n",
      "\n",
      "        [[-0.0961, -0.0159,  0.1070,  0.0081,  0.0922],\n",
      "         [-0.0859, -0.1024, -0.1002, -0.0764, -0.0575],\n",
      "         [-0.0020,  0.0323,  0.0384, -0.0291,  0.0323],\n",
      "         [-0.0884, -0.0826, -0.0143, -0.0699,  0.0784],\n",
      "         [-0.1066,  0.0144, -0.1136,  0.1069,  0.0435]],\n",
      "\n",
      "        [[ 0.0553, -0.0419, -0.0099, -0.0948, -0.0688],\n",
      "         [ 0.0023, -0.1136, -0.0485, -0.0658,  0.0608],\n",
      "         [-0.0482, -0.0460,  0.0321,  0.0239, -0.0422],\n",
      "         [-0.0965,  0.0671, -0.0280,  0.0786,  0.0731],\n",
      "         [ 0.0229,  0.0736, -0.0972,  0.0803, -0.0036]]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "加载后:  tensor([[[20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.]],\n",
      "\n",
      "        [[20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.]],\n",
      "\n",
      "        [[20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.],\n",
      "         [20191104., 20191104., 20191104., 20191104., 20191104.]]],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# ================================== update state_dict ===========================\n",
    "\n",
    "net_new = LeNet2(classes=2019)\n",
    "\n",
    "print(\"加载前: \", net_new.features[0].weight[0, ...])\n",
    "net_new.load_state_dict(state_dict_load) # 模型加载字典\n",
    "print(\"加载后: \", net_new.features[0].weight[0, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 断点续训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以在训练完每个epoch之后，保存下epoch,optimizer,net的信息。\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(checkpoint, CHECKPOINT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "checkpoint_interval = 5\n",
    "MAX_EPOCH = 10\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.01\n",
    "log_interval = 10\n",
    "val_interval = 1\n",
    "\n",
    "\n",
    "# ============================ step 1/5 数据 ============================\n",
    "\n",
    "split_dir = os.path.join(\"..\", \"..\", \"data\", \"rmb_split\")\n",
    "train_dir = os.path.join(split_dir, \"train\")\n",
    "valid_dir = os.path.join(split_dir, \"valid\")\n",
    "\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomGrayscale(p=0.8),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 构建MyDataset实例\n",
    "train_data = RMBDataset(data_dir=train_dir, transform=train_transform)\n",
    "valid_data = RMBDataset(data_dir=valid_dir, transform=valid_transform)\n",
    "\n",
    "# 构建DataLoder\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ============================ step 2/5 模型 ============================\n",
    "\n",
    "net = LeNet(classes=2)\n",
    "net.initialize_weights()\n",
    "\n",
    "# ============================ step 3/5 损失函数 ============================\n",
    "criterion = nn.CrossEntropyLoss()                                                   # 选择损失函数\n",
    "\n",
    "# ============================ step 4/5 优化器 ============================\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)                        # 选择优化器\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)     # 设置学习率下降策略\n",
    "\n",
    "# ============================ step 5/5 训练 ============================\n",
    "train_curve = list()\n",
    "valid_curve = list()\n",
    "\n",
    "start_epoch = -1\n",
    "for epoch in range(start_epoch+1, MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计分类情况\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "        # 打印训练信息\n",
    "        loss_mean += loss.item()\n",
    "        train_curve.append(loss.item())\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_loader), loss_mean, correct / total))\n",
    "            loss_mean = 0.\n",
    "\n",
    "    scheduler.step()  # 更新学习率\n",
    "    \n",
    "    '''\n",
    "    此段代码加入断点保存\n",
    "    '''\n",
    "    if (epoch+1) % checkpoint_interval == 0:\n",
    "\n",
    "        checkpoint = {\"model_state_dict\": net.state_dict(),\n",
    "                      \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                      \"epoch\": epoch}\n",
    "        path_checkpoint = \"./checkpoint_{}_epoch.pkl\".format(epoch)\n",
    "        torch.save(checkpoint, path_checkpoint)\n",
    "    \n",
    "    '''\n",
    "    此段代码加入断点保存\n",
    "    '''\n",
    "\n",
    "    if epoch > 5:\n",
    "        print(\"训练意外中断...\")\n",
    "        break\n",
    "\n",
    "    # validate the model\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "\n",
    "        correct_val = 0.\n",
    "        total_val = 0.\n",
    "        loss_val = 0.\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(valid_loader):\n",
    "                inputs, labels = data\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "                loss_val += loss.item()\n",
    "\n",
    "            valid_curve.append(loss.item())\n",
    "            print(\"Valid:\\t Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, j+1, len(valid_loader), loss_val/len(valid_loader), correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果需要从上次状态接着训练的话：\n",
    "\n",
    "      if resume:\n",
    "         # 恢复上次的训练状态\n",
    "         print(\"Resume from checkpoint...\")\n",
    "         checkpoint = torch.load(CHECKPOINT_FILE)\n",
    "         net.load_state_dict(checkpoint['model_state_dict'])\n",
    "         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "         initepoch = checkpoint['epoch']+1\n",
    "         #从上次记录的损失和正确率接着记录\n",
    "         dict = torch.load(ACC_LOSS_FILE)\n",
    "         loss_record = dict['loss']\n",
    "         acc_record = dict['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "checkpoint_interval = 5\n",
    "MAX_EPOCH = 10\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.01\n",
    "log_interval = 10\n",
    "val_interval = 1\n",
    "\n",
    "\n",
    "# ============================ step 1/5 数据 ============================\n",
    "\n",
    "split_dir = os.path.join(\"..\", \"..\", \"data\", \"rmb_split\")\n",
    "train_dir = os.path.join(split_dir, \"train\")\n",
    "valid_dir = os.path.join(split_dir, \"valid\")\n",
    "\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomGrayscale(p=0.8),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 构建MyDataset实例\n",
    "train_data = RMBDataset(data_dir=train_dir, transform=train_transform)\n",
    "valid_data = RMBDataset(data_dir=valid_dir, transform=valid_transform)\n",
    "\n",
    "# 构建DataLoder\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ============================ step 2/5 模型 ============================\n",
    "\n",
    "net = LeNet(classes=2)\n",
    "net.initialize_weights()\n",
    "\n",
    "# ============================ step 3/5 损失函数 ============================\n",
    "criterion = nn.CrossEntropyLoss()                                                   # 选择损失函数\n",
    "\n",
    "# ============================ step 4/5 优化器 ============================\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)                        # 选择优化器\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)     # 设置学习率下降策略\n",
    "\n",
    "\n",
    "# ============================ step 5+/5 断点恢复 ============================\n",
    "\n",
    "path_checkpoint = \"./checkpoint_4_epoch.pkl\"\n",
    "checkpoint = torch.load(path_checkpoint)\n",
    "\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "scheduler.last_epoch = start_epoch\n",
    "\n",
    "# ============================ step 5/5 训练 ============================\n",
    "train_curve = list()\n",
    "valid_curve = list()\n",
    "\n",
    "for epoch in range(start_epoch + 1, MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计分类情况\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "        # 打印训练信息\n",
    "        loss_mean += loss.item()\n",
    "        train_curve.append(loss.item())\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_loader), loss_mean, correct / total))\n",
    "            loss_mean = 0.\n",
    "\n",
    "    scheduler.step()  # 更新学习率\n",
    "\n",
    "    if (epoch+1) % checkpoint_interval == 0:\n",
    "\n",
    "        checkpoint = {\"model_state_dict\": net.state_dict(),\n",
    "                      \"optimizer_state_dic\": optimizer.state_dict(),\n",
    "                      \"loss\": loss,\n",
    "                      \"epoch\": epoch}\n",
    "        path_checkpoint = \"./checkpint_{}_epoch.pkl\".format(epoch)\n",
    "        torch.save(checkpoint, path_checkpoint)\n",
    "\n",
    "    # if epoch > 5:\n",
    "    #     print(\"训练意外中断...\")\n",
    "    #     break\n",
    "\n",
    "    # validate the model\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "\n",
    "        correct_val = 0.\n",
    "        total_val = 0.\n",
    "        loss_val = 0.\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(valid_loader):\n",
    "                inputs, labels = data\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "                loss_val += loss.item()\n",
    "\n",
    "            valid_curve.append(loss.item())\n",
    "            print(\"Valid:\\t Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, j+1, len(valid_loader), loss_val/len(valid_loader), correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
